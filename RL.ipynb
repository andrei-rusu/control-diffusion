{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9107e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchmetrics.functional import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215bbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62bf4250-292b-4b18-9938-1f6ba516a629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.8058e-01, 1.9612e-01, 9.8058e-07])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.tensor([1e5, 2e4, 1e-1])\n",
    "torch.nn.functional.normalize(t, dim=0, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f088375-fee0-471d-8f1f-364c251b9dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7961db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.observation_space=Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "env.observation_space.shape=(2,)\n",
      "env.action_space=Discrete(3)\n",
      "env.action_space.n=3\n"
     ]
    }
   ],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "env.seed(0)\n",
    "print(f'{env.observation_space=}\\n{env.observation_space.shape=}\\n{env.action_space=}\\n{env.action_space.n=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204d7ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_size, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q_state = nn.Linear(hidden_dim, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.elu(self.hidden1(state))\n",
    "        x = F.elu(self.hidden2(x))\n",
    "        x = self.q_state(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc69327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, maxlen):\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "        self.last_experience = None\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.last_experience = experience\n",
    "        \n",
    "    def sample(self, batch_size, **kwargs):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        samples = random.choices(self.buffer, k=sample_size-1)\n",
    "        samples.append(self.last_experience)\n",
    "        zipped = zip(*samples)\n",
    "        if torch.cuda.is_available():\n",
    "            tensors = map(lambda x: torch.tensor(x).cuda(), zipped)\n",
    "        else:\n",
    "            tensors = map(torch.tensor, zipped)\n",
    "        # all have importance of 1\n",
    "        return tensors, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b04174db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityReplayBuffer():\n",
    "    def __init__(self, maxlen):\n",
    "        self.buffer = np.zeros(maxlen, dtype=object)\n",
    "        self.priorities = np.zeros(maxlen, dtype=float)\n",
    "        self.importance = np.zeros(maxlen, dtype=float)\n",
    "        self.sum_priority = 0\n",
    "        self.max_importance = 0\n",
    "        # helps us iterate the head of the Turing Machine tape (fixed length of 'maxlen')\n",
    "        self.iterator = 0\n",
    "        # iterator will track the 'true' length of the buffer (instead of explicitly using a deque)\n",
    "        self.true_len = 0\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.true_len += 1\n",
    "        # the object's true size is the minimum between the true_len iterator and the internal buffer size (which starts at maxlen)\n",
    "        self.true_len = min(self.true_len, len(self.buffer))\n",
    "        iterator = self.iterator\n",
    "        self.buffer[iterator] = experience\n",
    "        prev_priority = self.priorities[iterator]\n",
    "        new_priority = 1.\n",
    "        self.priorities[iterator] = new_priority\n",
    "        new_importance = 1. / self.true_len\n",
    "        self.importance[iterator] = new_importance\n",
    "        self.sum_priority += new_priority - prev_priority\n",
    "        self.max_importance = max(self.max_importance, new_importance)\n",
    "        self.iterator = iterator + 1 if iterator < len(self.buffer) - 1 else 0\n",
    "        \n",
    "    def sample(self, batch_size, **kwargs):\n",
    "        len_buffer = self.true_len\n",
    "        sample_size = min(len_buffer, batch_size)\n",
    "        sample_probs = self.priorities[:len_buffer] / self.sum_priority\n",
    "        # we remember these in an obj attribute to use them at the set_priotities stage (which needs the TD error to be computed first)\n",
    "        self.sample_indices = np.random.default_rng().choice(range(len_buffer), size=sample_size, p=sample_probs)\n",
    "        samples = self.buffer[self.sample_indices]\n",
    "        importance = self.importance[self.sample_indices] / self.max_importance\n",
    "        \n",
    "        zipped = zip(*samples)\n",
    "        if torch.cuda.is_available():\n",
    "            tensors = map(lambda x: torch.tensor(x).cuda(), zipped)\n",
    "            importance = torch.tensor(importance).cuda()\n",
    "        else:\n",
    "            tensors = map(torch.tensor, zipped)\n",
    "            importance = torch.tensor(importance)\n",
    "        return tensors, importance\n",
    "    \n",
    "    def set_priorities(self, errors, priority_scale=0.7, offset=0.1):\n",
    "        for i,e in zip(self.sample_indices, errors):\n",
    "            prev_priority = self.priorities[i]\n",
    "            raw_new_priority = abs(e.item()) + offset\n",
    "            new_priority = self.priorities[i] = raw_new_priority ** priority_scale\n",
    "            self.sum_priority += new_priority - prev_priority\n",
    "            new_importance = self.importance[i] = 1 / (self.true_len * raw_new_priority)\n",
    "            self.max_importance = max(self.max_importance, new_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf75578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityReplayBufferInef():\n",
    "    def __init__(self, maxlen):\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "        self.priorities = deque(maxlen=maxlen)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(max(self.priorities, default=1))\n",
    "        \n",
    "    def get_probabilities(self, priority_scale=0.7):\n",
    "        scaled_priorities = np.array(self.priorities) ** priority_scale\n",
    "        sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "        return sample_probabilities\n",
    "    \n",
    "    def get_importance(self, probabilities):\n",
    "        importance = 1/len(self.buffer) * 1/probabilities\n",
    "        importance_normalized = importance / max(importance)\n",
    "        return importance_normalized\n",
    "        \n",
    "    def sample(self, batch_size, priority_scale=0.7):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        sample_probs = self.get_probabilities(priority_scale)\n",
    "        # we remember these in an obj attribute to use them at the set_priotities stage (which needs the TD error to be computed first)\n",
    "        self.sample_indices = random.choices(range(len(self.buffer)), k=sample_size, weights=sample_probs)\n",
    "        samples = np.array(self.buffer, dtype=object)[self.sample_indices]\n",
    "        importance = self.get_importance(sample_probs[self.sample_indices])\n",
    "        \n",
    "        zipped = zip(*samples)\n",
    "        if torch.cuda.is_available():\n",
    "            tensors = map(lambda x: torch.tensor(x).cuda(), zipped)\n",
    "            importance = torch.tensor(importance).cuda()\n",
    "        else:\n",
    "            tensors = map(torch.tensor, zipped)\n",
    "            importance = torch.tensor(importance)\n",
    "        return tensors, importance\n",
    "    \n",
    "    def set_priorities(self, errors, offset=0.1):\n",
    "        for i,e in zip(self.sample_indices, errors):\n",
    "            self.priorities[i] = abs(e.item()) + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9611b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, env, hidden_dim=100, gamma=.99, eps_decay=.99, lr=.001, priority_buffer=False, len_buffer=1000, batch_size=100, use_different_target=False):\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.q_network = QNetwork(self.state_dim, self.action_size, hidden_dim)\n",
    "        self.q_network_target = QNetwork(self.state_dim, self.action_size, hidden_dim) if use_different_target else self.q_network\n",
    "        if torch.cuda.is_available():\n",
    "            self.q_network = self.q_network.cuda()\n",
    "            self.q_network_target = self.q_network_target.cuda()\n",
    "        self.priority_buffer = priority_buffer\n",
    "        self.replay_buffer = PriorityReplayBufferInef(maxlen=len_buffer) if priority_buffer else ReplayBuffer(maxlen=len_buffer)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps = 1.\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state)\n",
    "        if torch.cuda.is_available():\n",
    "            state = state.cuda()\n",
    "        q_state = self.q_network(state)\n",
    "        action = random.randint(0, self.action_size - 1) if random.random() < self.eps else torch.argmax(q_state).item()\n",
    "        return action\n",
    "    \n",
    "    def train(self, state, action, next_state, reward, done):\n",
    "        # add to replay buffer, and sample a batch of transitions\n",
    "        self.replay_buffer.add((state, action, next_state, reward, done))\n",
    "        (states, actions, next_states, rewards, dones), importance = self.replay_buffer.sample(self.batch_size)\n",
    "        if torch.cuda.is_available():\n",
    "            states = states.cuda()\n",
    "            next_states = next_states.cuda()\n",
    "        # get Q values for all actions possible in current state\n",
    "        q_states = self.q_network(states)\n",
    "        # select Q value for the specific action that was taken (vectorized)\n",
    "        q_state_actions = q_states.gather(1, actions.view(-1, 1)).squeeze(dim=-1)\n",
    "        # get Q target from next_states (according to DQN this should be lagging behind, with no grad updates)\n",
    "        with torch.no_grad():\n",
    "            q_next_states = self.q_network_target(next_states)\n",
    "            q_next_states[dones] = 0\n",
    "            q_targets = rewards + self.gamma * torch.max(q_next_states, dim=1).values\n",
    "        self.optimizer.zero_grad()\n",
    "        errors = q_targets - q_state_actions\n",
    "        # the importance needs to be scaled by 'b' as per the paper\n",
    "        # 'b' starts at 0, which reflects normal updates at the beginning where mostly random actions are picked\n",
    "        # and then increases over time to 1, to account for the bias introduced due to priority sampling the experiences\n",
    "        importance_factor = importance ** (1 - self.eps) if self.priority_buffer else 1\n",
    "        loss = torch.mean(importance_factor * errors ** 2)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.priority_buffer:\n",
    "            # set the new priorities in the replay buffer based on the td errors\n",
    "            self.replay_buffer.set_priorities(errors)\n",
    "        if done:\n",
    "            # decay eps over time\n",
    "            self.eps = max(.1, self.eps_decay * self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d614d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import time\n",
    "\n",
    "priority_buffer = True\n",
    "use_different_target = True\n",
    "update_target_after = 30\n",
    "hidden_dim = 32\n",
    "eps_decay = .995\n",
    "gamma = .95\n",
    "maxlen = 1000\n",
    "agent = DQNAgent(env, hidden_dim=hidden_dim, gamma=gamma, priority_buffer=priority_buffer, len_buffer=maxlen, \n",
    "                 eps_decay=eps_decay, use_different_target=use_different_target)\n",
    "num_episodes = 800\n",
    "num_steps = 200\n",
    "events_range = range(num_steps) if num_steps else count()\n",
    "render = False\n",
    "print_reward_every = 10\n",
    "\n",
    "t1 = 0\n",
    "ep_rewards = 0\n",
    "try:\n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        if use_different_target and not ep % update_target_after:\n",
    "            agent.q_network_target.load_state_dict(agent.q_network.state_dict())\n",
    "\n",
    "        for t in events_range:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.train(state, action, next_state, reward, done)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        ep_rewards += total_reward\n",
    "        if not ep % print_reward_every:\n",
    "            t2 = time.time()\n",
    "            print(f'Episode {ep}, total reward: {total_reward}, average since last: {ep_rewards / print_reward_every}; computed in: {t2 - t1}')\n",
    "            t1 = t\n",
    "            ep_rewards = 0\n",
    "            \n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.eps = 0\n",
    "agent.q_network.eval()\n",
    "agent.q_network_target.eval()\n",
    "render = True\n",
    "try:\n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        for time in events_range:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        print(f'Episode {ep}, in {time} steps; total reward: {total_reward}')\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94820c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3f1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7266a079",
   "metadata": {},
   "source": [
    "<h1> Frozen lake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ac2443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "def run_episode(env, policy, episode_len=100):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    for t in range(episode_len):\n",
    "        # env.render()\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            # print('Epside finished after {} timesteps.'.format(t+1))\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def run_policy(env, policy, num_ep=200, monitor=False):\n",
    "    if monitor:\n",
    "        env = wrappers.Monitor(env, 'saved/frozenlake1', video_callable=lambda episode_id: True, force=True)\n",
    "    for _ in range(num_ep):\n",
    "        run_episode(env, policy)\n",
    "    env.close()    \n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, n_episodes=100):\n",
    "    total_rewards = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        total_rewards += run_episode(env, policy)\n",
    "    return total_rewards / n_episodes\n",
    "\n",
    "def gen_random_policy():\n",
    "    return np.random.choice(4, size=((16)))\n",
    "\n",
    "def crossover(policy1, policy2):\n",
    "    new_policy = policy1.copy()\n",
    "    for i in range(16):\n",
    "        rand = np.random.uniform()\n",
    "        if rand > 0.5:\n",
    "            new_policy[i] = policy2[i]\n",
    "    return new_policy\n",
    "\n",
    "def mutation(policy, p=0.05):\n",
    "    new_policy = policy.copy()\n",
    "    for i in range(16):\n",
    "        rand = np.random.uniform()\n",
    "        if rand < p:\n",
    "            new_policy[i] = np.random.choice(4)\n",
    "    return new_policy\n",
    "\n",
    "def main(n_steps=20, n_eval_steps=200):\n",
    "    random.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "    env = gym.make('FrozenLake-v1')\n",
    "    env.seed(0)\n",
    "    # env = wrappers.Monitor(env, '/tmp/frozenlake1', force=True)\n",
    "    ## Policy search\n",
    "    n_policy = 100\n",
    "    start = time.time()\n",
    "    policy_pop = [gen_random_policy() for _ in range(n_policy)]\n",
    "    for idx in range(n_steps):\n",
    "        policy_scores = [evaluate_policy(env, p) for p in policy_pop]\n",
    "        print('Generation %d : max score = %0.2f' %(idx+1, max(policy_scores)))\n",
    "        policy_ranks = list(reversed(np.argsort(policy_scores)))\n",
    "        elite_set = [policy_pop[x] for x in policy_ranks[:5]]\n",
    "        select_probs = np.array(policy_scores) / np.sum(policy_scores)\n",
    "        child_set = [crossover(\n",
    "            policy_pop[np.random.choice(range(n_policy), p=select_probs)], \n",
    "            policy_pop[np.random.choice(range(n_policy), p=select_probs)])\n",
    "            for _ in range(n_policy - 5)]\n",
    "        mutated_list = [mutation(p) for p in child_set]\n",
    "        policy_pop = elite_set\n",
    "        policy_pop += mutated_list\n",
    "    policy_score = [evaluate_policy(env, p) for p in policy_pop]\n",
    "    best_policy = policy_pop[np.argmax(policy_score)]\n",
    "\n",
    "    end = time.time()\n",
    "    print('Best policy score = %0.2f. Time taken = %4.4f'\n",
    "            %(np.max(policy_score), (end-start)))    \n",
    "\n",
    "    ## Evaluation\n",
    "    run_policy(env, best_policy, n_eval_steps, monitor=True)\n",
    "    return best_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d475fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 : max score = 0.20\n",
      "Generation 2 : max score = 0.30\n",
      "Generation 3 : max score = 0.60\n",
      "Generation 4 : max score = 0.66\n",
      "Generation 5 : max score = 0.79\n",
      "Best policy score = 0.84. Time taken = 13.1888\n"
     ]
    }
   ],
   "source": [
    "policy = main(n_steps=5, n_eval_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14c3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14f705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba822b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Policy iteration.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(env, v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "def compute_policy_v(env, policy, gamma=1.0):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
    "    max_iterations = 200000\n",
    "    gamma = 1.0\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(env, old_policy_v, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy\n",
    "\n",
    "\n",
    "def main_pi():\n",
    "    env_name  = 'FrozenLake8x8-v1'\n",
    "    env = gym.make(env_name)\n",
    "    optimal_policy = policy_iteration(env, gamma = 1.0)\n",
    "    scores = evaluate_policy(env, optimal_policy, gamma = 1.0)\n",
    "    print('Average scores = ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a893e252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 12.\n",
      "Average scores =  0.82\n"
     ]
    }
   ],
   "source": [
    "main_pi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5127ce7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1163c3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1da9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Q-Learning example using OpenAI gym MountainCar enviornment\n",
    "Author: Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "n_states = 40\n",
    "iter_max = 2000\n",
    "\n",
    "initial_lr = 1.0 # Learning rate\n",
    "min_lr = 0.003\n",
    "gamma = 1.0\n",
    "t_max = 200\n",
    "eps = 0.02\n",
    "\n",
    "def run_episode(env, policy=None, render=False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    try:\n",
    "        for _ in range(t_max):\n",
    "            if render:\n",
    "                env.render()\n",
    "            if policy is None:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                a,b = obs_to_state(env, obs)\n",
    "                action = policy[a][b]\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += gamma ** step_idx * reward\n",
    "            step_idx += 1\n",
    "            if done:\n",
    "                break\n",
    "    except:\n",
    "        env.close()\n",
    "    return total_reward\n",
    "\n",
    "def obs_to_state(env, obs):\n",
    "    \"\"\" Maps an observation to state \"\"\"\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    env_dx = (env_high - env_low) / n_states\n",
    "    a = int((obs[0] - env_low[0])/env_dx[0])\n",
    "    b = int((obs[1] - env_low[1])/env_dx[1])\n",
    "    return a, b\n",
    "\n",
    "def main_qlearn():\n",
    "    env_name = 'MountainCar-v0'\n",
    "    env = gym.make(env_name)\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    print ('----- using Q Learning -----')\n",
    "    q_table = np.zeros((n_states, n_states, 3))\n",
    "    for i in range(iter_max):\n",
    "        obs = env.reset()\n",
    "        total_reward = 0\n",
    "        ## eta: learning rate is decreased at each step\n",
    "        eta = max(min_lr, initial_lr * (0.85 ** (i//100)))\n",
    "        for j in range(t_max):\n",
    "            a, b = obs_to_state(env, obs)\n",
    "            if np.random.uniform(0, 1) < eps:\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                logits = q_table[a][b]\n",
    "                logits_exp = np.exp(logits)\n",
    "                probs = logits_exp / np.sum(logits_exp)\n",
    "                action = np.random.choice(env.action_space.n, p=probs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += (gamma ** j) * reward\n",
    "            # update q table\n",
    "            a_, b_ = obs_to_state(env, obs)\n",
    "            q_table[a][b][action] = q_table[a][b][action] + eta * (reward + gamma *  np.max(q_table[a_][b_]) - q_table[a][b][action])\n",
    "            if done:\n",
    "                break\n",
    "        if i % 100 == 0:\n",
    "            print('Iteration #%d -- Total reward = %d.' %(i+1, total_reward))\n",
    "    solution_policy = np.argmax(q_table, axis=2)\n",
    "    solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(100)]\n",
    "    print(\"Average score of solution = \", np.mean(solution_policy_scores))\n",
    "    # Animate it\n",
    "    run_episode(env, solution_policy, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0facce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- using Q Learning -----\n",
      "Iteration #1 -- Total reward = -200.\n",
      "Iteration #101 -- Total reward = -200.\n",
      "Iteration #201 -- Total reward = -200.\n",
      "Iteration #301 -- Total reward = -200.\n",
      "Iteration #401 -- Total reward = -200.\n",
      "Iteration #501 -- Total reward = -200.\n",
      "Iteration #601 -- Total reward = -200.\n",
      "Iteration #701 -- Total reward = -200.\n",
      "Iteration #801 -- Total reward = -200.\n",
      "Iteration #901 -- Total reward = -200.\n",
      "Iteration #1001 -- Total reward = -200.\n",
      "Iteration #1101 -- Total reward = -200.\n",
      "Iteration #1201 -- Total reward = -200.\n",
      "Iteration #1301 -- Total reward = -200.\n",
      "Iteration #1401 -- Total reward = -200.\n",
      "Iteration #1501 -- Total reward = -200.\n",
      "Iteration #1601 -- Total reward = -178.\n",
      "Iteration #1701 -- Total reward = -200.\n",
      "Iteration #1801 -- Total reward = -200.\n",
      "Iteration #1901 -- Total reward = -200.\n",
      "Average score of solution =  -195.94\n"
     ]
    }
   ],
   "source": [
    "main_qlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eae17d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
